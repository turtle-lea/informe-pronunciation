In order to analyse if the obtained results are statistically significant,
two different techiques are used in this work: \textit{Bootstrapping} and
\textit{McNemar's Test}.

Bootstrapping is a technique that models the underlying
distribution by using a resampling method on the available samples while McNemar's
Test is an statistical test used on paired data.

An advantage for McNemar's
over Bootstrapping is that it bases its comparisons on cross information using
exactly the same sample. McNemar's Test however, unlike Bootstrapping, assumes that samples
are independent, which is not true for the samples of the current work.

Both techniques focus on different statistical characteristics of the results and allow a
complementary analysis.

\subsection{Bootstrapping}

\textit{Bootstrapping} \cite{bootstrapping} is a general intuitive method applicable
to almost any kind of sample statistic and can be understood without much
knowledge of sampling distributions. It was introduced in 1979 by B. Efron and it has
been widely used in numerous areas since then.

Suppose it were possible to draw repeated samples
of the same size from the population of interest a large number of times. Then
a fairly good idea about the sampling distribution of a particular statistic from
the collection of its values arising from these repeated samples could be obtained.
This method doesn't make sense in practice since it assumes one has access to the
underlying distribution.
The idea behind \textit{Bootstrapping} is to use the data of a sample study at hand as a
``surrogate population'' for the purpose of approximating the sampling distribution of
a statistic. This is achieved by resampling with replacement from the sample data
at hand, creating a large number of alternative or duplicated sample sets known as
bootstrap samples. The performance metric (EER in our case)
is then computed on each of the bootstrap
samples (usually between 1-10 thousand). A histogram of these computed
values is referred to as the bootstrap distribution of the statistic.

In the current work, \textit{Bootstrapping} technique is used along with Confidence
Intervals in order to determine if the \textit{SVM} trained with the combined features
performs better than the one trained only with features derived from the \textit{GMM} adaptation.
Bootstrap samples are extracted from the \textit{test} data in order
to generate a distribution of $EER$ for both types of classifiers. After that,
a 95\% confidence interval is computed for each distribution by finding the interval
[$\hat{\theta}_{B1}, \hat{\theta}_{B2}$] that enclose the 95\% of the instances
composing each distribution.
If it is true that the \textit{SVM} trained with
the combined features performs better than the
\textit{SVM} trained with a single feature source, then its confidence
interval should be shifted to the left with respect to the confidence interval
of the latter system.
The bigger the overlap between both intervals, the
lesser the evidence of the results coming from different distributions, thus
resulting in less significant results.
