The first step in order to build the system is to extract features from the speech data. In this
work we base our features in the standard and widely used Mel Frecuency Cepstral Coefficients 
(MFCCs).

The presence or absence of particular frequencies at some instant are consequences of the shape of
the vocal tract at that instant. The disposition of the vocal tract during the 
release of air generates vibrations 
that determine which kind of sound comes out.
The choice of using power spectrum based features is motivated by
the way the human cochlea (the organ in the ear responsible for speech processing) works.
Depending on the location in the cochlea that vibrates (which wobbles small hairs), 
different nerves inform to the brain that certain frequencies are present. 
MFCCs performs a similar job carrying information about which frequencies are present at
some instant. 


\subsubsection{Window Size and Shift}

Spectral Density is meant to be computed over an interval where the audio signal is not changing.
To satisfy that condition, each phone utterance instance is divided into frames of 25 $ms$. 
This is the most standard value for window size when computing MFCCs since 
each frame contains enough samples to get a reliable spectral estimate, and at the same time is
short enough to minimize the signal changes.

For the window's shift or step the most standard value of 10 $ms$ was also selected, allowing some
overlap to the frames.


\subsubsection{Spectral Density and Mel Filterbanks}

The next step in order to compute the MFCCs is to obtain the spectral density for each frame,
that describes the power as fuction of the different frequencies that make up the signal.
The Fast Fourier Transform technique (FFT) is used in order to do so. 

Not all the information provided by the spectral density is useful to describe the source
of the signal. The cochlea can't discern between two closely spaced frequencies and the
effect becomes more pronounced as the frecuencies increase. For that reason, frequencies
that are close enough are grouped together and summed up to get an idea of how much energy
exists in various frequency regions. This is achieved by using Mel Filterbanks: 
the first filter is very narrow and gives information about how much energy exists near 0 Hz.
As the frequencies get higher, the filters get wider and variations become harder to 
detect. The Mel scale describe exactly how to space the filterbanks and how wide to make them:

\begin{equation}
M(f)=1125*ln(1 + \frac{f}{700})
\end{equation}

Once computed the filterbank energies, the logarithm of them is taken. This is also motivated by
the human hearing thus we don't perceive loudness on a linear way, but following an exponential
scale. So large variations in energy may not sound all that different if the sound is loud to 
begin with.

Finally, the DCT of the log filterbank energies is computed. This is performed because of  
two main reasons. On the one hand, it helps to decorrelate the energy of the filterbanks
in response to the overlap between them. On the other hand, only 13 of the DCT coefficients
are kept. This is because the higher DCT coefficients represent fast changes in the filterbank
energies and it turns out that these fast changes actually degrades performance 
when they are included as features to build speech related systems.


\subsubsection{Deltas and Deltas-Deltas}

The MFCC feature vector describes only the power spectral envelope of a single frame, but speech
would also have information in the dynamics.	 
Calculating differential and acceleration coefficients often leads to small improvements  
in the performance of speech related systems when they are appended to the original feature vector.
