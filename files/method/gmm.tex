MFCCs are modeled with a Gaussian Mixture Model, which is a parametric probability density
function represented as a weighted sum of Gaussian component densities. GMMs are often
used in biometric systems, specially in speaker recognition systems, due to their
capability of representing a large class of sample distributions. One of the powerful
attributes of the GMM is its ability to form smooth aproximations to arbitrarily shaped
densities \cite{gmm_reynolds}.

The densitiy function of a GMM is defined as:

\begin{equation}
	p(x) = \sum_{k=1}^{K}\pi_{k} \mathcal{N}(x|\mu_{k},\,\Sigma_{k})
\end{equation}

Each Gaussian Density $\mathcal{N}(x|\mu_{k},\,\Sigma_{k})$ is called a component of the mixture
and has its own mean $\mu_{k}$ and variance $\Sigma_{k}$. The parameters $\pi_{k}$ are called
mixing coefficients and they can be thought as the prior probability of picking the $k^{th}$
component \cite{gmm_bishop}.

Each Gaussian component is determined by the formula:

\begin{equation}
	\mathcal{N}(x|\mu_{k},\,\Sigma_{k}) = \frac{1}{(2\pi)^{(D/2)}|\Sigma|^{1/2}} exp \big\{ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\}
\end{equation}

Where $D$ is the dimension of the features space (39 MFCCs in this case), the $D$-dimensional
vector $\mu$ is the mean, the $DxD$ matrix $\Sigma$ is called the covariance and $|\Sigma|$
denotes the determinant of $\Sigma$.

\subsection{Expectation Maximization Algorithm}

Given the training vectors, the objective is to estimate the parameters $\lambda$ of the GMM
that in some sense best matches the distribution of the data. The aim of the estimation is
to find the model parameters which maximize the likelihood of the given training data. For a
sequence of $N$ training vectors $X=\{x_{1}, x_{2}, \dotsc x_{N}\}$ the GMM likelihood
(assuming independence between vectors) can be written as:

\begin{equation}
	\label{eq:likelihoodGMM}
	p(X|\lambda) = \prod_{n=1}^{N}p(x_{n}|\lambda)
\end{equation}

The parameters $\lambda$ that maximizes \ref{eq:likelihoodGMM} are obtained by using an iterative
algorithm called Expectation Maximization. The basic idea of the EM algorithm is,
beginning with an initial model $\lambda$ (i.e random initialized), estimate a new model
$\bar{\lambda}$ such that $p(X|\bar{\lambda}) \geq p(X|\lambda)$. The new model then becomes
the initial model for the next iteration and the process is repeated until some convergence
model is reached.

Each iteration involves two steps that give the algorithm its name. The first step is the \emph{Expectation} step, where the posterior probabilities $\gamma(z_{nk})$ are computed.
These posteriors represents the \emph{responsibility} that component $k$ takes for
'explaining' the observation $x_{n}$.

\begin{equation}
	\gamma(z_{nk}) = \frac{\pi_{k} \mathcal{N}(x_{n}|\mu_{k},\,\Sigma_{k})}{\sum_{j=1}^{K}\pi_{k} \mathcal{N}(x_{n}|\mu_{j},\,\Sigma_{j})}
	\label{eq:expectationStep}
\end{equation}

During the Maximization step, the parameters are re-estimated using the current responsibilities:

\begin{equation}
	\mu_{k}^{new} = \frac{1}{N_{k}}\sum_{n=1}^{N}\gamma(z_{nk})x_{n}
\end{equation}

\begin{equation}
	\Sigma_{k}^{new} = \frac{1}{N_{k}}\sum_{n=1}^{N}\gamma(z_{nk})(x_{n} - \mu_{k}^{new})(x_{n} - \mu_{k}^{new})^{T}
\end{equation}

\begin{equation}
	\pi_{k}^{new} = \frac{\sum_{n=1}^{N}\gamma(z_{nk})}{N}
\end{equation}

After the Maximization step, the log likelihood of the new model is evaluated and the convergence
criterion is evaluated for either the parameters or the log likelihood:

\begin{equation}
	ln \ p(X|\mu, \Sigma, \pi) = \sum_{n=1}^{N}ln\Big\{\sum_{k=1}^{K}\pi_{k}\mathcal{N}(x_{n}|\mu_{k},\,\Sigma_{k})\Big\}
\end{equation}


If the convergence criterion is not satisfied, the algorithm returns to \ref{eq:expectationStep}
to start a new iteration of the Expectation and Maximization steps.

\subsection{\textit{Universal Background Model} Adaptation}

% A \textit{GMM} model trained using \textit{MFCCs} as input features is the chosen classifier for
% the first experiment of the current work. The classifiers for the are trained with features
In the present work, as in the previous works of the current line of investigation
\cite{detection_phone_level_mispronunciation_learning, main}, the \textit{GMMs} are derived
by using a techinque called \textit{GMM-UBM} \textit{Universal Background Model} \cite{ubm_adaptation}.

The \textit{UBM} is a large class-independent \textit{GMM} intended to represent the whole possible
distribution of features. In a \textit{GMM-UBM} system, the
\textit{GMM} is derived by adapting the parameters of the \textit{UBM} using the particular
instances according to the custom desired model and a form of
\textit{Maximum a Posteriori} (\textit{MAP}) estimation. This provides a tighter coupling
between the particular models and the \textit{UBM} that produces better performance than
using decoupled models, and it can be specially usefull for the models that has a small number
of training instances.
