\subsection{Future Work}

As it was mentioned in section \ref{section:motivation}, a new dataset of
argentinian children reading english phrases is currently being collected
and annotated. In the future, we plan to verify our findings on
this dataset, which will be significantly bigger than the one used in this work.

% While the current work was being written (after the experiments were carried out)
% the Argentine children's speech database that was previously mentioned in section
% \ref{section:motivation} was
% collected and annotated. In the near future we plan to verify our findings on
% this dataset.

% The database contains speech
% from Argentine children between 6 and 12 years old, which are currently learning
% english. As \textit{Computer-Assisted Language Learning} (CALL) systems
% for kids should preferably be based on short speech units (phones or words) because
% of the difficulty of children in pronouncing longer segments, the combined features
% explored in the current thesis, which are phone-level features, could be helpful
% in developing the CALL system. For this reason, it would be interesting to test
% the combined features along with the SVM model against the new database, in search
% of potential gains.

Due to the increasing number of highly effective DNN-based methods that can be found in
many fields (including pronunciation scoring,
as it was described in the Previous Work section \ref{section:prev_work}),
it would be worthwile to
keep exploring DNN-based solutions to phone-based pronunciation scoring, using
some training algorithms and features that we have predefined as starting point.
% pronunciation assessment at phone level.

% Below are listed some of the systems that would be interesting to explore
% along with their features:

% \begin{itemize}
%   \item Feed Forward DNN trained on supervectors or dynamic features (a set of features per phone utterance)
%   \item Feed Forward DNN trained on the MFCCs of each frame plus context (a set of features per frame)
%   \item Long Short-Term Memory (LSTM) DNN trained on the MFCCs of each frame (a set of features per frame)
%   \item Feed Forward or LSTM trained on more generic features, such as the spectrum of frequencies of a spectrogram
% \end{itemize}
Finally, a different approach
that is worth studying, even though it is not standard in the literature
on pronunciation assessment,
is the usage of \textit{Detection Cost Function} (DCF) % as the performance measure
to explore an alternative performance metric
instead of \textcolor{red}{Equal Error Rate}.
% instead of keeping the \textit{Equal Error Rate} (EER) as the chosen metric.}
DCF combines both \textit{False Positive Rate} and \textit{False Negative Rate}
according to a given cost, which is an input parameter.
As consequence, the system can focus on different operating points
by varying the value of the cost parameter and thus prioritizing,
in different degrees, one measure over the other one.
% one measure can be prioritized over the other one.}
In the context of pronunciation assessment in
CALL systems, labeling a correctly pronounced utterance as incorrect should be
penalized stronger than labeling a mispronounced utterance as correct, in order
to avoid discouraging the students. Because of those reasons,
we think that DCF
\textcolor{red}{would be a more suitable metric for this scenario.}
% would help to find a more suitable metric for this scenario.
% This metric focuses on a different operating point that we think is more suitable for
% This metric focuses on a different operating point that we think is more suitable for
