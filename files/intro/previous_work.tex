In this section we will be reviewing representative works in pronunciation assessment
at phone level, which is the topic of interest in this thesis.

One of the simplest ways of scoring a phone pronunciation found in the literature 
is to use segment duration scores (\cite{pronunciation_scoring_instruction, pronunciation_scoring_phone_segments_instruction}), which are obtained through the following
procedure. First, a discrete duration distribution is generated for each phone
using the native training data. 
Then, phone durations in frames for nonnative utterances are obtained 
and its value is normalized to compensate for rate of speech. 
Finally, phone-segment-duration scores are computed as 
the log-probability of the normalized duration, using the discrete duration 
distributions previously mentioned. Phone durations are obtained from Viterbi alignments.
In the same works, other two methods that use \textit{Hidden Markov Models} (HMMs) to obtain 
spectral matches and compute pronunciation scores are tested. 
Phonetic time alignments for
non-native utterances are generated from an HMM-based speech recognition system trained
with native instances. In order to do that, the text pronounced by the student 
must be known. This is achieved by eliciting speech in a
constrained way, such as reading a predefined text.
From these phonetic segmentations two 
probabilistic mesasures based on HMMs are computed as scores: log-likelihood and 
log-posterior probabilities. The underlying asumption is that the logarithm of the likelihood
of the speech data, computed by the Viterbi algorithm using the HMMs trained with speech from native
speakers is a good mesasure of the similarity between the students's
speech and native-sounding speech.

For each phone segment the log-likelihood score \^{l} is computed as:
\begin{equation}
\hat{l} = \frac{1}{d} \sum_{t=t_{0}}^{t_{0}+d-1} log \ p(y_{t}|q_{i})
\end{equation}
where $p(y_{t}|q_{i})$ is the likelihood at the time $t$ with observation vector $y_{t}$
given the phone class $q_{i}$, $d$ is the duration in frames of the phone segment 
and $t_{0}$ is the starting frame index of the phone segment. Duration normalization is done to 
eliminate the dependency of the pronunciation score on the duration of the phone.

Alternatively, log-posterior scores can be computed for each time $t$:

\begin{equation}
P(q_{i}|y_{t}) = \frac{p(y_{t}|q_{i})P(q_{i})}{\sum\limits_{j=1}^{M} p(y_{t}|q_{j})P(q_{j})}
\end{equation}
Likelihood in the numerator is computed through a forced recognition phase by using a known 
ortographic transcription of the speech signal. On the other hand, 
the sum over $j$ runs over a set of context-independent models for all phone classes. $P(q_{i})$
represents the prior probability of the phone class $q_{i}$. 

Finally, the posterior score $\hat{\rho}$ for the phone segment is defined as:
\begin{equation}
\hat{\rho} = \frac{1}{d}\sum_{t=t_{0}}^{t_{0}+d-1} log \ P(q_{i}|y_{t})
\end{equation}

To test each method, a database of nonnative read speech is transcribed and scored for 
pronunciation quality by expert human raters. Log-posterior probabilities achieves the
highest correlation with human ratings, outperforming log-likelihood and normalized duration
scores.

A very similar approach to log-posterior probabilities named \textit{Goodness of Pronunciation}
(GOP) was developed at the same time as log-posteriors and
is used in some works \cite{gop_1, gop_2, gop_3}. The quality of 
pronunciation for any phone $p$ is defined to be the duration-normalized log of the posterior
probability that the speaker uttered phone $p$ given the corresponding vector of observations
$y=\{y_{t_{0}}, \dotsc y_{t_{0}+d-1} \}$

\begin{equation}
GOP(p) = \left| log \ \left(\frac{p(y|q)P(p))}{\sum_{q \in Q}p(y|q)P(q))}\right) \right| \Biggm/ d 
\end{equation}

The likelihoods $p(y|q)$ are obtained from the recognizer as a sum of the likelihoods over all
observations in the phone.
The difference between GOP scores and log-posterior score technique previously mentioned 
is that in GOP, the likelihood for both numerator and every phone in the denominator is
computed at segment level as a sum of log-likelihoods per frame over the 
duration of the phone $p$, while in Equation (1.2) log-posterior probabilities are computed
at the frame level and averaged over the segment's length.

So far, aforementioned methods are based on confidence measures 
obtained from Automatic Speech Recognition (ASR) systems. 
Scores measure how closely the utterance of the speaker matches the recognizer's
acoustic model. Mismatches result in low confidence scores, which provide a profile of the 
speakers' production erorrs. Nevertheless, the accuracy of the assessment based on these
confidence scores can be quite low \cite{landmark_svm}. 
In addition, ASR systems based on HMMs are both time-consuming to train and extremely vulnerable to
acoustic interference and variation in speaking style, and the conventional methods for
enhancing ASR performance often require enormous amounts of data collection and annotation,
as well as extensive training on representative material.
For those reasons, other types of classifiers are explored in many works, specially after
the 1990s. 

\textit{Support Vector Machines} (SVMs) are a preferred choice 
due to their excellent generalization capability and suitability for 2-class classification
problems. Moreover, in contrast to the confidence models described above, which do not take into
account misspronounced data, SVMs are trained directly to discriminate the two classes of interest.
Of course, this poses a new requirement: both correctly and incorrectly-pronounced data should be
available for training.

In \cite{detection_mispronunciation_dutch_vowel}, SVMs are trained with different types of
features to discriminate between good and mispronounced vowels in Dutch: log-posterior-based 
scores obtained from HMMs, MFCCs and a set of 
phonetic features (first three formants, pitch and intensity). Despite the existence of nonnative-speech  
databases for Dutch language, they were considered too small for the purpose of the research. 
For that reason, phonemic 
substitutions that induce vocalic errors are simulated by artificially introducing them in the native corpus.
The results show that the best 
results are obtained by using MFCCs as features of the SVM models, followed by confidence-measure-based 
scores and finally phonetic
features, though substantial improvements can be obtained by combining them.

In a different work \cite{svm_space_models}, SVM is used as the classifier and the
log-likelihood ratios between all the acoustic models and the model corresponding to the given
phone are employed as features for the classifier. In other words, given the observation
$y$, the feature vector for the classification problem is computed as:

[$LLR(y|q,q_{1}), \ LLR(y|q,q_{2}) \dotsc LLR(y|q, q_{M})$],

where $q$ is the canonical label of the observation being pronounced and {$q_{1}, q_{2} \dotsc q_{M}$}
represent the set of acoustic models of all the phones. The reason for choosing these features is
that $q$ can be mispronounced as any other phone and the likelihood ratio is a powerful
method to detect this kind of mispronunciation. Classifiers trained with this kind of features, however, 
can only effectively deal with a phone mispronounced as another phone. This kind of acoustic models is 
less effective for partially changed mispronunciations, that are the most frequent mispronunciations
in practice. In order to detect all kinds of mispronunciation, a technique called  
\textit{Pronunciation Space Models} (PSMs) is introduced. The idea is to represent pronunciation variations
of different proficiency levels. The traditional phone-based model $q$ is expanded to 
\{$q_{1}, q_{2} \dotsc q_{K}$\} representing $K$ types of pronunciation variations, ranging from perfect or
totally wrong pronunciations, where $K$ is a tunable parameter. PSMs are built from an unsupervised method
using the posterior probabilities obtained from the acoustic models for each phone $q$.
Experiments are carried out on the 
Mandarin mispronunciation detection task for native Chinese speakers with various dialect accents, 
and data labeled as 'correct' or not by experienced human annotators is used to train and test the models.
Results show that SVM based on log-likelihood ratio features outperforms GOP scores (used as baseline), and some
imporvements are obtained when adding PSMs.

Relying on labeled nonnative speech data leads to more flexible models that yields better performance
when working on pronunciation assessment of a specific combination of L1 and L2. On the one hand,
the set of common pronunciation errors tend to be typical for a given L1, L2 pair so 
models that specifically focus on these errors can be trained. On the other hand, it provides more 
flexibility when assessing nonnative utterances close to native-sounding, in a similar way that
an annotator would do. However, the cost of nonnative database collection and annotation is very
high and not always feasible.

Finally, in \cite{landmark_svm}\cite{landmark_svm_2} a landmark based 
SVM is introduced and compared with a confideence scoring method over 10 phonemes where 
L2 English learners, whose native language is Korean, make frequent errors. 
Landmark theory relies on the fact that humans can perceive distinctive
features using only spectral features extracted from the time frame including and adjacent to
a landmark (sudden signal change). A particular SVM for each phoneme is trained. Vowel SVMs are 
trained using one or more frames from the vowel center. Frames from both onset and offset
(prevocalic and postvocalic position) are selected and used in the training of consonant
SVMs. Confidence score method shows a similar performance to SVM, though by combining 
the two scores, statistically significant improvement are achieved for almost all phonemes.

A different strategy for discrimination of Dutch velar fricative /x/ versus the velar plosive
/k/ is studied in \cite{lda_weigelt}. Latent Discriminant Analysis (LDA) over two different sets
of features are explored and compared with previous approaches: Aforementioned GOP scores and
Weigelt. This last algorithm is based on three measures that can be easily obtained:
log root-mean-square (rms) energy, the derivative of log rms energy (\textit{Rate of Rise} or
ROR) and zero-crossing rate. Weigelt algorithm discriminate plosives from fricatives by using
ROR values, found on the fact that the release of the burst of the plosives causes an abrupt
rise in amplitude and therefore yielding higher values compared with fricatives.
On the other hand, in LDA weights are assigned to each features to 
find the linear combination of features
which best separates the classes. Selecting most relevant features turns out to be an
advantage compared to other classifiers. The two LDA methods yielded the best performance
scores followed by GOP and Weigelt.

A second group of papers are based on L1 strategies. A sufficiently large annotated nonnative
database is required in order to follow approaches of this kind.
In \cite{detection_phone_level_mispronunciation_learning}, a phonetically labeled nonnative
database is used to train two different \textit{Gaussian mixture models} (GMMs) for each phone
class: one model is trained with the "correct" native-like pronunciations of a phone, while the
other model is trained with the "mispronounced" or nonnative pronunciations of the same phone.
In the evaluation phase, for each phone segment $q_{i}$, a length-normalized log-likelihood ratio
$LLR(q_{i})$ score is computed for the phone segment by using the "mispronounced" and "correct"
pronunciation models $\lambda_{M}$ and $\lambda_{C}$ respectively:
\begin{equation}
LLR(q_{i}) = \frac{1}{d}\sum_{t=t_{0}}^{t_{0}+d-1} [log \ p(y_{t}|q_{i}, \lambda_{M}) - log \ p(y_{t}|q_{i}, \lambda_{C})]
\end{equation}
The normalization by $d$ allows definition of unique thresholds for the LLR for each phone class, 
independent of the length of the segments.

Finally, a recent work \cite{main} explores another method that also uses 
log-likelihood ratio based on  GMMs (eq 1.4), except that the models for each 
class ("correct" and  "mispronounced") are obtained by adaptation. In the same work, 
a discriminative system based
on \textit{Support Vector Machines} (SVM) is developed producing good results. Features 
for this classifier are obtained by adapting class-independent GMMs to each particular 
instance of the target phone and extracting both means and weights of the resulting 
Gaussian Mixture.
These methods proved to be superior to the mispronunciation detection system based of 
LLR of independently trained GMMs (eq 1.4), which had been previously shown to 
outperform the standard method tha uses phone log-posterior as scores, yielding
results comparable to state-of-the-art techniques.
Both systems previously described are used as baselines in the work here 
presented so a detailed explanation will be provided in next sections.
